{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edfd15f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bddad8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ALL_DATA_DIR = '/media/windows/'\n",
    "training = ALL_DATA_DIR + 'training/'\n",
    "validation = ALL_DATA_DIR + 'validation/'\n",
    "scaled = ALL_DATA_DIR + 'scaled/'\n",
    "scaled_wo_rotate = ALL_DATA_DIR + \"scaled_wo_rotate/\"\n",
    "\n",
    "scaled_wo_rotate_training = scaled_wo_rotate + \"training/\"\n",
    "scaled_wo_rotate_validation = scaled_wo_rotate + \"validation/\"\n",
    "\n",
    "datFile = training + \"shape_predictor_68_face_landmarks.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c83fcb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "img_height = 224\n",
    "img_width = 224 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51196fff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    final_image = keras.applications.xception.preprocess_input(image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c5f64bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50947 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_s = tf.keras.utils.image_dataset_from_directory(scaled_wo_rotate_validation, labels = 'inferred',\n",
    "            image_size=(img_height,img_width), batch_size=None, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd9f883",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 399839 files belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "training_s = tf.keras.utils.image_dataset_from_directory(scaled_wo_rotate_training, labels = 'inferred',\n",
    "            image_size=(img_height,img_width), batch_size=None, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b9f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = validation_s.batch(batch_size).prefetch(1)\n",
    "train = training_s.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4f1243",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "validation_ds = validation_s.map(preprocess).batch(batch_size).prefetch(1)\n",
    "training_ds = training_s.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30329c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('0825_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1c2897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593/1593 [==============================] - 199s 122ms/step - loss: 0.5502 - accuracy: 0.8100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.550179660320282, 0.809998631477356]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8bf5bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12495/12495 [==============================] - 1562s 125ms/step - loss: 0.3448 - accuracy: 0.8767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3448373079299927, 0.8766578435897827]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(training_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8af546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = keras.models.load_model('0901_mobilenet.h5')\n",
    "model2 = keras.models.load_model('0902_mobile_no_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "673cd085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593/1593 [==============================] - 50s 30ms/step - loss: 0.6359 - accuracy: 0.7731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6359350681304932, 0.7731367945671082]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41bb3b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593/1593 [==============================] - 47s 28ms/step - loss: 0.7757 - accuracy: 0.7240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7757318615913391, 0.7239680290222168]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5070eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12495/12495 [==============================] - 372s 30ms/step - loss: 0.4756 - accuracy: 0.8260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47561851143836975, 0.8260249495506287]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aabca7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12495/12495 [==============================] - 374s 30ms/step - loss: 0.6056 - accuracy: 0.7772\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6055830717086792, 0.7771602869033813]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
